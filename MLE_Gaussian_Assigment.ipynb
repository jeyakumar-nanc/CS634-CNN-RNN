{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLE_Gaussian_Assigment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMbCFujD3rLvYNRqMSVxM2d"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fEbFJwIBNa5"
      },
      "source": [
        "**Exercise 1: MLE of a Gaussian p_model(50 points)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mi3IFw6geb3U"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy.stats\n",
        "import warnings\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62JZfyQFu0VH"
      },
      "source": [
        "#partial derivative w.r.t to mean x-mu/sigma_2\n",
        "def partial_deriv_mean(data, est_mean, est_var):\n",
        "  warnings.filterwarnings(\"ignore\")\n",
        "  return (np.sum(data - est_mean))/est_var**2\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsmQPEeU-D9M"
      },
      "source": [
        "#partial derivative w.r.t to variance\n",
        "def partial_deriv_var(data, est_mean, est_var,N):\n",
        "  warnings.filterwarnings(\"ignore\")\n",
        "  return (((np.sum((data - est_mean)**2))/est_var**2)-N)/2*(est_var**2);\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDB4qmRmiPLr"
      },
      "source": [
        "def gradient_descent(data, est_mean, est_var, learning_rate, epochs):\n",
        "  #prev_w = np.array([est_mean, est_var])\n",
        "  for k in range(epochs):\n",
        "    for x in data:\n",
        "      d_mean = partial_deriv_mean(x, est_mean, est_var)\n",
        "      d_var = partial_deriv_var(x, est_mean, est_var,len(data))\n",
        "  \n",
        "      est_mean = est_mean - (learning_rate*(-d_mean))\n",
        "      est_var = est_var - (learning_rate*(-d_var))\n",
        "\n",
        "  print(\"MLE for mean gaussian parameters - %.2f, variance - %.2f\" % (est_mean,est_var)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MH40OyevxVxt",
        "outputId": "78257530-a437-4955-fb08-4414d519fe73"
      },
      "source": [
        "data = [4, 5, 7, 8, 8, 9, 10, 5, 2, 3, 5, 4, 8, 9]\n",
        "gradient_descent(data,0,1,0.01,100000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLE for mean gaussian parameters - 6.22, variance - 2.43\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoOD3pi7BgZA"
      },
      "source": [
        "To estimate the optimal values of gaussian distribution used stochastic gradient descent algorithm.\n",
        "1. started with initial values for mean and variance as 0 , 1, learning rate 0.01\n",
        "2. computed the partial derivates of mean and variance starting with random values of mean and variance\n",
        "3. Plug and chug the derivate values to calculate the gradient\n",
        "4. Estimated mean and varianance is derived from the formula est val = est val - loss fn ; where loss fn = learning rate * (partial derivate of mean/variance)\n",
        "5. Repeated the process until we get very small value for loss function. The value of est_mean and est_var at the end of the iterations are optimal values for gaussian distribution\n",
        "\n",
        "For our data, MLE for gaussian parameters mean - 6.22, variance - 2.43"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOcEew8ZTsvw"
      },
      "source": [
        "**Exercise 2: MLE of a Gaussian p_model for a regression problem (50 points)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00XXf0dDTrPs"
      },
      "source": [
        "def linear_gradient_descent(x,y,learning_rate,epochs):  \n",
        "  plt.scatter(x, y) #plot the data\n",
        "  m_curr = 0\n",
        "  c_curr = 0\n",
        "  n = float(len(x))\n",
        "  \n",
        "  for i in range(epochs):\n",
        "    y_pred = m_curr * x + c_curr    #linear equation y = mx+c\n",
        "    #at each step calculating the predicted values of y\n",
        "    md = (-2/n) *sum(x*(y-y_pred)) #partial derivative of m\n",
        "    cd = (-2/n) *sum(y-y_pred) #partial derivative of m\n",
        "    m_curr = m_curr - (learning_rate * md)\n",
        "    c_curr = c_curr - (learning_rate * cd)\n",
        "    \n",
        "  plt.plot(x,y_pred)    #plot the model that fits the data closely\n",
        "  \n",
        "  print (\"MLE for linear regression m  -> {}, c -> {}\".format(m_curr,c_curr))\n",
        "  \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "e1w6vZP44sTu",
        "outputId": "34e25d1f-4b79-49a7-8fd2-1ed8a7caa2d7"
      },
      "source": [
        "x = np.array([8, 16, 22, 33, 50, 51])\n",
        "y = np.array([5, 20, 14, 32, 42, 58])\n",
        "\n",
        "linear_gradient_descent(x,y,0.0001,1000)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLE for linear regression m  -> 0.9720324681885764, c -> -0.0915723644677935\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S398us0njNyz"
      },
      "source": [
        "Linear equation => y = mx+c\n",
        "Applying gradient descent algorithm to determine the optimal value for m and c.\n",
        "1. Initially let m = 0 and c = 0 and learning rate as 0.0001. This controls how much the value of m changes with each step\n",
        "2. Calculated the partial derivative of the loss function with respect to m, and plug in the current values of x, y, m and c in it to obtain the derivative value D.\n",
        "3. Similarly found the partial derivative with respect to c\n",
        "4. Updated current value of m and c using the formula:\n",
        "m_curr = m_curr - (learning_rate * derivative of m)\n",
        "c_curr = c_curr - (learning_rate * derivative of c)\n",
        "5. Repeated the process until we got very small value for loss function. The value of m and c at the end of the iterations are optimal values\n",
        "In our example, it is m  -> 0.9720324681885764, c -> -0.0915723644677935"
      ]
    }
  ]
}