{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Logistic Regression and SVMs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeyakumar-nanc/MachineLearning/blob/main/Logistic_Regression_and_SVMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tajfsk_7JY3E"
      },
      "source": [
        "**Note to grader:** Each question consists of parts, e.g. Q1(i), Q1(ii), etc. Each part must be graded  on a 0-4 scale, following the standard NJIT convention (A:4, B+: 3.5, B:3, C+: 2.5, C: 2, D:1, F:0). \n",
        "The total score must be re-scaled to 100 -- that should apply to all future assignments so that Canvas assigns the same weight on all assignments. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Grader's area\n",
        "import numpy as np\n",
        "M = np.zeros([10,10])\n",
        "maxScore = 0\n"
      ],
      "metadata": {
        "id": "zPnHTf9MfT5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SArgW_Vq-uTh"
      },
      "source": [
        "# **Assignment 3**\n",
        "\n",
        "The goal in this assignment to work a little more with Python, do some practice with logistic regression, reflect on how it can fail to work on linearly separable data. You will also work with a support vector classifier. All that, still on \"toy\" data sets. \n",
        "\n",
        "We will work with the first 'real' data sets in the next assignment. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlFM4hig-uTj"
      },
      "source": [
        "## **Preparation Steps**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3alYkjM-uTk"
      },
      "source": [
        "# Import all necessary python packages\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTVa-fum-uTt",
        "outputId": "32dbf0e6-fe88-4594-fb01-a5caf63886de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# ### Reading-in the Iris data\n",
        "\n",
        "s = os.path.join('https://archive.ics.uci.edu', 'ml',\n",
        "                 'machine-learning-databases', 'iris','iris.data')\n",
        "s = s.replace(\"\\\\\",\"/\");\n",
        "print('URL:', s)\n",
        "df = pd.read_csv(s,header=None,encoding='utf-8')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URL: https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqzyfFCC-uTz",
        "outputId": "8dc0753f-1497-40ac-b9f3-e7595786fe17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "# select setosa and versicolor for binary classification\n",
        "y = df.iloc[0:100, 4].values\n",
        "y = np.where(y == 'Iris-setosa', -1, 1)\n",
        "\n",
        "# extract sepal length and petal length\n",
        "X = df.iloc[:100, [0, 2]].values\n",
        "\n",
        "# plot data\n",
        "plt.scatter(X[:50, 0], X[:50, 1],\n",
        "            color='red', marker='o', label='setosa')\n",
        "plt.scatter(X[50:100, 0], X[50:100, 1],\n",
        "            color='blue', marker='x', label='versicolor')\n",
        "\n",
        "plt.xlabel('sepal length [cm]')\n",
        "plt.ylabel('petal length [cm]')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "\n",
        "# plt.savefig('images/02_06.png', dpi=300)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYEgTzp0s5BN"
      },
      "source": [
        "# function for visualizing decision regions\n",
        "\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "\n",
        "def plot_decision_regions(X, y, classifier, resolution=0.02):\n",
        "  \n",
        "    # setup marker generator and color map\n",
        "    markers = ('s', 'x', 'o', '^', 'v')\n",
        "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
        "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
        "\n",
        "    # plot the decision surface\n",
        "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
        "                           np.arange(x2_min, x2_max, resolution))\n",
        "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
        "    Z = Z.reshape(xx1.shape)\n",
        "    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n",
        "    plt.xlim(xx1.min(), xx1.max())\n",
        "    plt.ylim(xx2.min(), xx2.max())\n",
        "\n",
        "    # plot class examples\n",
        "    for idx, cl in enumerate(np.unique(y)):\n",
        "        plt.scatter(x=X[y == cl, 0], \n",
        "                    y=X[y == cl, 1],\n",
        "                    alpha=0.8, \n",
        "                    c=colors[idx],\n",
        "                    marker=markers[idx], \n",
        "                    label=cl, \n",
        "                    edgecolor='black')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------------\n",
        "-----------------------\n",
        "-----------------------\n"
      ],
      "metadata": {
        "id": "phGY3AEGn3dz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pe9kKR3J-uUA"
      },
      "source": [
        "## <font color = 'blue'> **Question 1. Practice with logistic regression** </font>\n",
        "\n",
        "Let's first load the textbook's implementation of logistic regression with gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSRTXCwz-uUA"
      },
      "source": [
        "class LogisticRegressionGD(object):\n",
        "    \"\"\"Logistic Regression Classifier using gradient descent.\n",
        "\n",
        "    Parameters\n",
        "    ------------\n",
        "    eta : float\n",
        "      Learning rate (between 0.0 and 1.0)\n",
        "    n_iter : int\n",
        "      Passes over the training dataset.\n",
        "    random_state : int\n",
        "      Random number generator seed for random weight\n",
        "      initialization.\n",
        "\n",
        "\n",
        "    Attributes\n",
        "    -----------\n",
        "    w_ : 1d-array\n",
        "      Weights after fitting.\n",
        "    loss_ : list\n",
        "      Logistic loss function value in each epoch.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, eta=0.05, n_iter=100, random_state=1):\n",
        "        self.eta = eta\n",
        "        self.n_iter = n_iter\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\" Fit training data.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like}, shape = [n_examples, n_features]\n",
        "          Training vectors, where n_examples is the number of examples and\n",
        "          n_features is the number of features.\n",
        "        y : array-like, shape = [n_examples]\n",
        "          Target values.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "\n",
        "        \"\"\"\n",
        "        rgen = np.random.RandomState(self.random_state)\n",
        "        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])\n",
        "        self.loss_ = []\n",
        "\n",
        "        for i in range(self.n_iter):\n",
        "            net_input = self.net_input(X)\n",
        "            output = self.activation(net_input)\n",
        "            errors = (y - output)\n",
        "            self.w_[1:] += self.eta * X.T.dot(errors)\n",
        "            self.w_[0] += self.eta * errors.sum()\n",
        "            \n",
        "            # compute the logistic `loss` \n",
        "            loss = -y.dot(np.log(output)) - ((1 - y).dot(np.log(1 - output)))\n",
        "            self.loss_.append(loss)\n",
        "        return self\n",
        "    \n",
        "    def net_input(self, X):\n",
        "        \"\"\"Calculate net input\"\"\"\n",
        "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
        "\n",
        "    def activation(self, z):\n",
        "        \"\"\"Compute logistic sigmoid activation\"\"\"\n",
        "        return 1. / (1. + np.exp(-np.clip(z, -250, 250)))\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Return class label after unit step\"\"\"\n",
        "        return np.where(self.net_input(X) >= 0.0, 1, 0)\n",
        "        # equivalent to:\n",
        "        # return np.where(self.activation(self.net_input(X)) >= 0.5, 1, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMYBgs4E-uUF"
      },
      "source": [
        "Below you can see the first 3 data points of the data set, all labeled as 'setosa'. Let's set the numerical value for 'setosa' to 1. (i.e. y = 1). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyoLJVet-uUG",
        "outputId": "9d0f631b-3b2d-43e0-e2f6-0cc71f4330ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X[0:3]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.1, 1.4],\n",
              "       [4.9, 1.4],\n",
              "       [4.7, 1.3]])"
            ]
          },
          "metadata": {},
          "execution_count": 376
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPbGfqmf-uUK"
      },
      "source": [
        "\n",
        "Suppose the initial weights of the logistic neuron are w0=0.1, w1=-0.2, w2=0.1\n",
        "\n",
        "<font color = 'blue'> **Q1-1**.  </font> Write the weights after processing data points 0,1,2, with $\\eta=0.1$ and show your calculations. This is similar to the previous assignment, only done now for the logistic neuron. You can also use *LogisticRegressionGD* to check your calculations. <br>\n",
        "\n",
        "<font color = 'blue'> **Q1-2**.  </font> Given our data $X$, let $X_{d=2}$ and $X_{d=3}$ be the quadratic and cubic features. Using code from the notebook on polynomial regression, generate $X_{d=2}$ and $X_{d=3}$\n",
        "\n",
        "<font color = 'blue'> **Q1-3**.  </font> Using *LogisticRegressionGD* fit $X$, $X_{d=2}$ and $X_{d=3}$. Here you should set $\\eta = 0.1$ and $n_{iter}>1000$. For each of these three cases, report the loss function value for the model computed by *LogisticRegressionGD*.\n",
        "Here it is expected that the loss value decreases as $d$ increases. \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q1-1. Write the weights after processing data points 0,1,2, with  η=0.1  and show your calculations. \n",
        "#This is similar to the previous assignment, only done now for the logistic neuron. \n",
        "#You can also use LogisticRegressionGD to check your calculations.\n",
        "# your calculations and code go here\n",
        "x_sample = X[0:3]\n",
        "#print(x_sample)\n",
        "y_sample = np.ones([1,3]).reshape(x_sample.shape[0],1)\n",
        "#print(y_sample)\n",
        "eta = 0.1\n",
        "w = np.array([0.1,-0.2,0.1])\n",
        "\n",
        "#y_init = -1\n",
        "for i, item in enumerate(x_sample):  \n",
        " \n",
        "  net_input = np.dot(item, w[1:]) + w[0]\n",
        "  output = 1. / (1. + np.exp(-np.clip(net_input, -250, 250))) #sigmoid activation function\n",
        "  errors = (y_sample[i] - output)\n",
        "  w[1:] += eta * x_sample[i].T*(errors)\n",
        "  w[0] += eta * errors.sum()\n",
        "  print(w)\n"
      ],
      "metadata": {
        "id": "0wYFal09ID7Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54b54854-6452-4f7b-e52e-368f1ed5c92e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.16856801 0.14969686 0.19599522]\n",
            "[0.19213665 0.26518321 0.22899132]\n",
            "[0.20711645 0.33558826 0.24846505]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q1-2. Given our data  X , let  Xd=2  and  Xd=3  be the quadratic and cubic features.\n",
        "# Using code from the notebook on polynomial regression, generate  Xd=2  and  Xd=3\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "quadratic = PolynomialFeatures(degree=2)\n",
        "cubic = PolynomialFeatures(degree=3)\n",
        "\n",
        "# fit features\n",
        "X_fit = np.arange(X.min(), X.max(), 1)[:, np.newaxis]\n",
        "\n",
        "X_quad = quadratic.fit_transform(X)\n",
        "print(\"****x quad****\")\n",
        "#print(X_quad)\n",
        "print(X_quad.shape)\n",
        "\n",
        "X_cubic = cubic.fit_transform(X)\n",
        "print(\"****x cubic****\")\n",
        "#print(X_cubic)\n",
        "print(X_cubic.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tn5njVbtPstq",
        "outputId": "e3c49132-abfe-4830-a2a2-dc729a451e73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****x quad****\n",
            "(100, 6)\n",
            "****x cubic****\n",
            "(100, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Q1-3. Using LogisticRegressionGD fit  X ,  Xd=2  and  Xd=3 . Here you should set  η=0.1  and  niter>1000 . \n",
        "#For each of these three cases, report the loss function value for the model computed by LogisticRegressionGD.\n",
        "# Here it is expected that the loss value decreases as  d  increases.\n",
        "_iter =1001\n",
        "_eta = 0.00001\n",
        "lgr = LogisticRegressionGD(_eta,_iter)\n",
        "\n",
        "X_fit = np.arange(X.min(), X.max(), 1)[:, np.newaxis]\n",
        "#y = np.ones(X.shape[0])*1\n",
        "\n",
        "lgr_lnr = lgr.fit(X, y)\n",
        "y_lin_fit = lgr_lnr.predict(X)\n",
        "print(f\"linear - {lgr_lnr.loss_}\")\n",
        "\n",
        "lgr_quad = lgr.fit(X_quad, y)\n",
        "y_quad_fit = lgr_quad.predict(quadratic.fit_transform(X))\n",
        "print(f\"quadratic - {lgr_quad.loss_}\")\n",
        "\n",
        "lgr_cubic = lgr.fit(X_cubic, y)\n",
        "y_cubic_fit = lgr_cubic.predict(cubic.fit_transform(X))\n",
        "print(f\"cubic - {lgr_cubic.loss_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jcIXrTMQTIp",
        "outputId": "cc8234b2-ee9b-4ebd-e5e3-04c653389a12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "linear - [68.73506119844845, 68.21798308429935, 67.70896027940486, 67.20782924133975, 66.71442921898117, 66.22860224971635, 65.7501931533134, 65.27904952267494, 64.81502171168657, 64.35796282036472, 63.90772867750062, 63.4641778209897, 63.02717147602749, 62.596573531345776, 62.17225051365468, 61.75407156044914, 61.34190839133016, 60.935635277984346, 60.53512901295772, 60.14026887735254, 59.750936607569415, 59.367016361209764, 58.988394682247495, 58.61496046557219, 58.24660492100034, 57.883221536844765, 57.52470604312718, 57.17095637451317, 56.821872633043604, 56.47735705073154, 56.13731395208901, 55.801649716643276, 55.47027274149808, 55.14309340399084, 54.820024024493286, 54.500978829398946, 54.185873914337485, 53.87462720765251, 53.56715843417613, 53.26338907933088, 52.96324235358655, 52.66664315729663, 52.37351804593704, 52.08379519576702, 51.797404369930035, 51.51427688501037, 51.23434557805929, 50.95754477410273, 50.68381025414064, 50.413079223646804, 50.14529028157628, 49.88038338988625, 49.61829984357485, 49.358982241241414, 49.1023744561702, 48.84842160793904, 48.5970700345532, 48.34826726510378, 48.10196199294947, 47.858104049419474, 47.61664437803505, 47.37753500924611, 47.14072903567922, 46.90618058789247, 46.67384481063248, 46.443677839588304, 46.21563677863651, 45.989679677571715, 45.76576551031626, 45.543854153602474, 45.32390636612106, 45.10588376812849, 44.889748821506586, 44.6754648102669, 44.46299582149279, 44.25230672671164, 44.0433631636899, 43.83613151864327, 43.63057890885466, 43.426673165692165, 43.224382818019656, 43.02367707599224, 42.824525815229094, 42.62689956135624, 42.4307694749116, 42.236107336605045, 42.04288553292592, 41.851077042090864, 41.66065542032463, 41.471594788466604, 41.283869818896186, 41.09745572276991, 40.91232823756329, 40.7284636149108, 40.54583860873714, 40.36443046367331, 40.18421690375071, 40.00517612136727, 39.8272867665191, 39.65052793629135, 39.47487916460257, 39.30032041219634, 39.126832056874434, 38.95439488396569, 38.78299007702522, 38.61259920875812, 38.443204232162515, 38.27478747188663, 38.107331615794585, 37.940819706735994, 37.77523513451443, 37.61056162804964, 37.44678324772925, 37.28388437794474, 37.12184971980774, 36.96066428404169, 36.800313384045, 36.640782629121034, 36.48205791787126, 36.32412543174713, 36.166971628756826, 36.010583237323424, 35.85494725028995, 35.700050919068545, 35.54588174792938, 35.392427488426435, 35.23967613395628, 35.087615914446864, 34.93623529117286, 34.7855229516945, 34.635467804916814, 34.48605897626621, 34.33728580298149, 34.18913782951652, 34.04160480305158, 33.8946766691109, 33.74834356728355, 33.60259582704514, 33.457423963677954, 33.312818674286824, 33.168770833908546, 33.02527149171244, 32.88231186728973, 32.73988334702962, 32.59797748057984, 32.45658597738961, 32.31570070333287, 32.175313677409996, 32.03541706852577, 31.896003192341915, 31.75706450820233, 31.618593616129044, 31.480583253887488, 31.34302629411891, 31.205915741538742, 31.069244730198903, 30.93300652081274, 30.797194498140936, 30.661802168436846, 30.52682315694994, 30.392251205485803, 30.258080170021376, 30.124304018374012, 29.99091682792316, 29.857912783383284, 29.725286174626753, 29.593031394555585, 29.461142937020824, 29.329615394788245, 29.19844345754964, 29.067621909978016, 28.937145629826272, 28.807009586067778, 28.677208837078137, 28.547738528857135, 28.41859389328974, 28.289770246445386, 28.16126298691449, 28.03306759418154, 27.90517962703349, 27.777594722003073, 27.650308591845825, 27.523317024050257, 27.396615879380317, 27.270201090449284, 27.14406866032452, 27.018214661162265, 26.892635232871736, 26.76732658180784, 26.64228497949194, 26.517506761359982, 26.392988325537125, 26.268726131638616, 26.144716699596046, 26.020956608508445, 25.89744249551767, 25.77417105470759, 25.651139036026336, 25.52834324423122, 25.405780537855858, 25.283447828198735, 25.161342078333075, 25.039460302137147, 24.91779956334486, 24.796356974616046, 24.67512969662584, 24.554114937173136, 24.43330995030715, 24.312712035472124, 24.19231853666952, 24.07212684163739, 23.95213438104656, 23.83233862771309, 23.712737095826935, 23.59332734019614, 23.474106955506436, 23.35507357559578, 23.236224872743527, 23.117558556973965, 22.999072375373807, 22.880764111423442, 22.762631584341484, 22.644672648442466, 22.526885192507365, 22.409267139166598, 22.291816444295304, 22.174531096420598, 22.05740911614052, 21.94044855555451, 21.823647497705043, 21.707004056030257, 21.5905163738273, 21.474182623726193, 21.358001007174003, 21.241969753928984, 21.12608712156463, 21.010351394983342, 20.894760885939466, 20.779313932571615, 20.66400889894402, 20.54884417459656, 20.43381817410374, 20.318929336641766, 20.204176125564153, 20.089557027985375, 19.975070554372415, 19.86071523814413, 19.746489635278124, 19.63239232392526, 19.518421904031243, 19.40457699696544, 19.290856245156792, 19.17725831173633, 19.063781880186646, 18.950425653997698, 18.837188356329285, 18.72406872967958, 18.61106553555995, 18.49817755417588, 18.385403584113636, 18.27274244203288, 18.160192962364853, 18.047753997016233, 17.935424415078273, 17.82320310254147, 17.711088962015317, 17.599080912453196, 17.487177888882403, 17.375378842138918, 17.263682738607184, 17.152088559964383, 17.040595302929578, 16.929201979017236, 16.817907614295223, 16.706711249147187, 16.595611938039188, 16.484608749290537, 16.373700764848703, 16.262887080068253, 16.152166803493756, 16.041539056646553, 15.931002973815222, 15.820557701849946, 15.710202399960304, 15.599936239516813, 15.48975840385587, 15.37966808808813, 15.269664498910291, 15.159746854420256, 15.049914383935242, 14.940166327813444, 14.830501937278537, 14.720920474247386, 14.611421211160579, 14.502003430816147, 14.392666426205952, 14.283409500354974, 14.17423196616341, 14.065133146251432, 13.956112372806626, 13.847168987434127, 13.738302341009163, 13.62951179353228, 13.520796713986988, 13.412156480199714, 13.303590478702382, 13.195098104597083, 13.086678761423197, 12.978331861026646, 12.870056823431474, 12.761853076713564, 12.65372005687636, 12.545657207728823, 12.437663980765429, 12.329739835048038, 12.221884237089913, 12.114096660741527, 12.006376587078403, 11.898723504290714, 11.79113690757474, 11.683616299026234, 11.576161187535348, 11.46877108868351, 11.361445524641809, 11.254184024071257, 11.146986122024437, 11.039851359849102, 10.932779285092966, 10.825769451410382, 10.71882141847036, 10.611934751866153, 10.505109023026266, 10.39834380912702, 10.291638693006384, 10.184993263079331, 10.078407113254455, 9.971879842852111, 9.865411056523595, 9.759000364171934, 9.652647380873729, 9.546351726802339, 9.440113027152329, 9.33393091206499, 9.227805016555257, 9.121734980439593, 9.015720448265178, 8.909761069240071, 8.803856497164666, 8.698006390364053, 8.59221041162155, 8.486468228113333, 8.38077951134395, 8.275143937082984, 8.169561185302602, 8.06403094011625, 7.958552889718133, 7.853126726323744, 7.747752146111301, 7.642428849164062, 7.53715653941364, 7.431934924584077, 7.326763716136897, 7.221642629216904, 7.116571382598957, 7.011549698635427, 6.906577303204568, 6.801653925659645, 6.69677929877885, 6.591953158716024, 6.4871752449520255, 6.382445300247042, 6.277763070593426, 6.173128305169367, 6.068540756293304, 5.964000179378909, 5.859506332890893, 5.755058978301397, 5.650657880047051, 5.546302805486757, 5.4419935248600435, 5.337729811246067, 5.233511440523225, 5.129338191329458, 5.025209845023003, 4.921126185643896, 4.817086999875947, 4.713092077009319, 4.6091412089037025, 4.505234189951951, 4.401370817044459, 4.2975508895337455, 4.193774209199942, 4.090040580216467, 3.986349809116465, 3.8827017047596044, 3.7790960782993857, 3.675532743150953, 3.5720115149594527, 3.468532211568675, 3.3650946529904218, 3.26169866137403, 3.1583440609766527, 3.055030678133658, 2.951758341229816, 2.848526880670569, 2.7453361288539586, 2.6421859201428965, 2.5390760908377814, 2.4360064791495795, 2.3329769251733605, 2.2299872708620203, 2.1270373600006263, 2.024127038180989, 1.921256152776591, 1.8184245529180636, 1.7156320894686985, 1.6128786150006604, 1.5101639837713332, 1.4074880517000654, 1.304850676345227, 1.2022517168816496, 1.0996910340783899, 0.9971684902767262, 0.8946839493685914, 0.7922372767752268, 0.6898283394261888, 0.587457005738635, 0.4851231455969689, 0.3828266303326373, 0.28056733270435785, 0.17834512687860027, 0.07615988841026677, -0.025988505776226134, -0.12810017740573798, -0.23017524687081803, -0.3322138332498632, -0.4342160543251019, -0.5361820266002191, -0.6381118653177893, -0.7400056844764649, -0.8418635968479151, -0.9436857139934673, -1.0454721462806198, -1.1472230028992279, -1.2489383918774877, -1.3506184200977351, -1.4522631933119818, -1.5538728161572202, -1.6554473921705508, -1.7569870238040544, -1.8584918124395386, -1.9599618584029486, -2.0613972609786906, -2.162798118423673, -2.2641645279812295, -2.365496585894787, -2.4667943874213325, -2.5680580268447706, -2.669287597489026, -2.7704831917309214, -2.8716449010130525, -2.9727728158562563, -3.0738670258720795, -3.1749276197749765, -3.275954685394412, -3.3769483096867, -3.477908578746778, -3.5788355778197634, -3.6797293913123355, -3.780590102804034, -3.8814177950582938, -3.9822125500334273, -4.082974448893381, -4.183703572018409, -4.28439999901553, -4.385063808728919, -4.485695079250082, -4.586293887927884, -4.686860311378576, -4.787394425495492, -4.88789630545873, -4.988366025744751, -5.088803660135611, -5.189209281728388, -5.289582962944209, -5.38992477553731, -5.490234790603932, -5.590513078591048, -5.690759709305059, -5.790974751920258, -5.8911582749873155, -5.991310346441509, -6.091431033610956, -6.191520403224672, -6.291578521420519, -6.391605453753087, -6.49160126520141, -6.591566020176618, -6.691499782529528, -6.791402615557963, -6.891274582014191, -6.991115744112127, -7.090926163534462, -7.190705901439715, -7.290455018469146, -7.390173574753685, -7.489861629920565, -7.589519243100149, -7.689146472932329, -7.788743377573166, -7.888310014701206, -7.987846441523812, -8.0873527147834, -8.186828890763554, -8.286275025295188, -8.385691173762353, -8.485077391108279, -8.584433731841164, -8.683760250039843, -8.783056999359578, -8.88232403303751, -8.981561403898294, -9.080769164359422, -9.179947366436714, -9.279096061749485, -9.378215301525856, -9.47730513660786, -9.576365617456581, -9.675396794157066, -9.774398716423384, -9.873371433603443, -9.972314994683831, -10.071229448294543, -10.170114842713689, -10.26897122587211, -10.367798645357976, -10.466597148421188, -10.565366781977964, -10.664107592615094, -10.762819626594345, -10.861502929856675, -10.960157548026459, -11.05878352641568, -11.157380910027946, -11.255949743562635, -11.354490071418757, -11.453001937699035, -11.551485386213646, -11.649940460484151, -11.748367203747243, -11.846765658958471, -11.945135868795903, -12.04347787566381, -12.141791721696206, -12.240077448760353, -12.338335098460353, -12.436564712140488, -12.534766330888605, -12.632939995539587, -12.73108574667856, -12.829203624644165, -12.927293669531762, -13.025355921196653, -13.123390419257234, -13.221397203097952, -13.319376311872507, -13.417327784506814, -13.515251659701878, -13.61314797593689, -13.711016771471986, -13.808858084351135, -13.906671952405025, -14.00445841325369, -14.102217504309383, -14.199949262779239, -14.297653725667908, -14.395330929780247, -14.492980911723844, -14.590603707911669, -14.688199354564517, -14.785767887713584, -14.883309343202793, -14.980823756691429, -15.078311163656329, -15.175771599394398, -15.273205099024828, -15.370611697491501, -15.467991429565185, -15.565344329845805, -15.662670432764655, -15.7599697725866, -15.857242383412155, -15.954488299179662, -16.05170755366747, -16.14890018049579, -16.246066213128955, -16.343205684877283, -16.440318628899156, -16.537405078202934, -16.634465065648847, -16.73149862395103, -16.82850578567928, -16.925486583260987, -17.0224410489829, -17.11936921499303, -17.216271113302362, -17.313146775786613, -17.409996234187968, -17.506819520116895, -17.603616665053693, -17.700387700350227, -17.797132657231593, -17.893851566797686, -17.990544460024836, -18.087211367767445, -18.18385232075939, -18.280467349615705, -18.3770564848341, -18.473619756796364, -18.57015719576982, -18.66666883190898, -18.763154695256777, -18.859614815746102, -18.956049223201124, -19.05245794733874, -19.148841017769904, -19.245198464000936, -19.341530315434966, -19.437836601373096, -19.53411735101577, -19.630372593464088, -19.72660235772095, -19.822806672692362, -19.918985567188667, -20.015139069925787, -20.11126720952627, -20.207370014520592, -20.303447513348306, -20.399499734359136, -20.495526705814125, -20.59152845588674, -20.687505012663998, -20.78345640414749, -20.879382658254542, -20.975283802819156, -21.07115986559311, -21.167010874247012, -21.26283685637125, -21.358637839476966, -21.454413850997174, -21.550164918287585, -21.645891068627627, -21.741592329221373, -21.837268727198513, -21.932920289615215, -22.028547043455056, -22.124149015629918, -22.219726232980886, -22.315278722279064, -22.41080651022647, -22.506309623456893, -22.601788088536708, -22.6972419319657, -22.792671180177855, -22.88807585954222, -22.983455996363624, -23.07881161688348, -23.17414274728059, -23.269449413671843, -23.364731642113, -23.459989458599416, -23.555222889066798, -23.650431959391817, -23.74561669539296, -23.840777122831135, -23.935913267410385, -24.031025154778547, -24.12611281052793, -24.221176260195996, -24.31621552926599, -24.411230643167517, -24.506221627277323, -24.60118850691979, -24.696131307367615, -24.791050053842355, -24.885944771515074, -24.98081548550692, -25.07566222088974, -25.170485002686554, -25.265283855872234, -25.360058805373995, -25.45480987607192, -25.549537092799603, -25.644240480344564, -25.73892006344885, -25.83357586680955, -25.92820791507925, -26.022816232866578, -26.117400844736746, -26.21196177521191, -26.306499048771855, -26.401012689854216, -26.495502722855147, -26.589969172129727, -26.68441206199236, -26.778831416717352, -26.873227260539156, -26.967599617653033, -27.061948512215313, -27.156273968343886, -27.250576010118643, -27.344854661581817, -27.439109946738476, -27.533341889556862, -27.62755051396878, -27.72173584387002, -27.815897903120764, -27.910036715545868, -28.004152304935335, -28.09824469504464, -28.19231390959508, -28.28635997227415, -28.38038290673587, -28.47438273660119, -28.568359485458252, -28.662313176862774, -28.756243834338335, -28.85015148137677, -28.94403614143848, -29.037897837952656, -29.13173659431771, -29.22555243390154, -29.319345380041778, -29.413115456046164, -29.506862685192772, -29.600587090730397, -29.69428869587872, -29.787967523828677, -29.88162359774274, -29.975256940755038, -30.068867575971904, -30.162455526471827, -30.256020815305973, -30.349563465498274, -30.443083500045724, -30.536580941918714, -30.63005581406113, -30.723508139390688, -30.816937940799157, -30.910345241152584, -31.003730063291574, -31.097092430031324, -31.19043236416219, -31.283749888449524, -31.377045025634185, -31.47031779843263, -31.563568229537093, -31.65679634161588, -31.750002157313443, -31.84318569925075, -31.936346990025385, -32.02948605221168, -32.12260290836106, -32.215697581002104, -32.30877009264071, -32.40182046576047, -32.49484872282261, -32.58785488626634, -32.68083897850893, -32.77380102194589, -32.86674103895122, -32.95965905187744, -33.0525550830559, -33.14542915479678, -33.23828128938942, -33.33111150910231, -33.42391983618339, -33.51670629286008, -33.609470901339485, -33.702213683808566, -33.79493466243419, -33.88763385936331, -33.98031129672326, -34.07296699662159, -34.165600981146426, -34.25821327236655, -34.35080389233144, -34.44337286307157, -34.53592020659838, -34.62844594490443, -34.720950099963574, -34.81343269373102, -34.905893748143505, -34.998333285119365, -35.09075132655859, -35.183147894343136, -35.275523010336784, -35.3678766963854, -35.46020897431703, -35.55251986594185, -35.644809393052626, -35.73707757742433, -35.829324440814645, -35.92155000496382, -36.013754291594864, -36.10593732241366, -36.19809911910893, -36.29023970335248, -36.382359096799185, -36.47445732108711, -36.5665343978376, -36.658590348655316, -36.75062519512838, -36.84263895882846, -36.9346316613107, -37.02660332411404, -37.11855396876111, -37.210483616758324, -37.302392289596, -37.39428000874844, -37.48614679567395, -37.57799267181498, -37.66981765859806, -37.761621777434, -37.853405049717914, -37.94516749682924, -38.03690914013184, -38.12863000097407, -38.22033010068881, -38.312009460593565, -38.40366810199043, -38.49530604616628, -38.5869233143927, -38.67851992792611, -38.77009590800779, -38.861651275864006, -38.953186052705924, -39.044700259729694, -39.13619391811666, -39.227667049033144, -39.31911967363074, -39.41055181304618, -39.501963488401486, -39.59335472080394, -39.68472553134618, -39.77607594110622, -39.867405971147505, -39.9587156425189, -40.05000497625478, -40.14127399337511, -40.23252271488531, -40.3237511617765, -40.414959355025424, -40.506147315594475, -40.59731506443175, -40.68846262247115, -40.77959001063223, -40.87069724982048, -40.96178436092714, -41.052851364829365, -41.143898282390126, -41.23492513445841, -41.32593194186906, -41.41691872544297, -41.50788550598697, -41.598832304293964, -41.68975914114287, -41.780666037298715, -41.87155301351257, -41.96242009052167, -42.053267289049344, -42.14409462980513, -42.23490213348475, -42.32568982077009, -42.41645771232926, -42.50720582881661, -42.597934190872806, -42.688642819124695, -42.779331734185504, -42.87000095665469, -42.96065050711809, -43.05128040614789, -43.14189067430258, -43.232481332127044, -43.32305240015255, -43.413603898896795, -43.504135848863804, -43.59464827054411, -43.68514118441461, -43.77561461093872, -43.86606857056624, -43.95650308373348, -44.046918170863194, -44.13731385236467, -44.22769014863365, -44.318047080052395, -44.408384666989676, -44.49870292980077, -44.58900188882753, -44.679281564398295, -44.76954197682794, -44.859783146417904, -44.95000509345624, -45.0402078382175, -45.130391400962736, -45.22055580193971, -45.31070106138262, -45.40082719951241, -45.49093423653645, -45.581022192648746, -45.67109108802992, -45.76114094284716, -45.85117177725425, -45.94118361139162, -46.03117646538623, -46.121150359351645, -46.211105313388096, -46.30104134758234, -46.39095848200783, -46.48085673672449, -46.57073613177897, -46.66059668720451, -46.75043842302087, -46.84026135923452, -46.93006551583846, -47.01985091281232, -47.10961757012234, -47.19936550772137, -47.289094745548766, -47.378805303530626, -47.46849720157955, -47.55817045959474, -47.64782509746201, -47.73746113505371, -47.82707859222884, -47.91667748883296, -48.00625784469817, -48.09581967964324, -48.18536301347338, -48.27488786598047, -48.364394256942845, -48.45388220612551, -48.54335173328, -48.63280285814437, -48.72223560044325, -48.81164997988771, -48.901046016175464, -48.990423728990784, -49.079783138004316, -49.16912426287336, -49.25844712324168, -49.347751738739554, -49.4370381289837, -49.52630631357748, -49.615556312110606, -49.70478814415927, -49.794001829286266, -49.88319738704073, -49.97237483695835, -50.06153419856116, -50.150675491357816, -50.23979873484325, -50.32890394849891, -50.41799115179265, -50.507060364178756, -50.59611160509793, -50.68514489397726, -50.77416025023024, -50.86315769325683, -50.952137242443214, -51.04109891716213, -51.13004273677251, -51.218968720619806, -51.30787688803568, -51.39676725833827, -51.48563985083193, -51.57449468480745, -51.66333177954188, -51.752151154298524, -51.84095282832714, -51.92973682086362, -52.01850315113026, -52.107251838335586, -52.19598290167431, -52.2846963603276, -52.37339223346267, -52.46207054023315, -52.55073129977873, -52.63937453122548, -52.72800025368559, -52.8166084862575, -52.90519924802581, -52.99377255806137, -53.08232843542113, -53.17086689914832, -53.25938796827214, -53.34789166180816, -53.43637799875798, -53.52484699810938, -53.61329867883621, -53.70173305989849]\n",
            "quadratic - [74.08405215089387, 63.336345171628686, 55.19094214402788, 48.170103170597784, 41.70295370222541, 35.55829954949411, 29.63607884888605, 23.889596053436563, 18.294833700836705, 12.837706547143853, 7.508578075814285, 2.2998574139407317, -2.795056697172292, -7.782233966286189, -12.667405038021311, -17.45603776685009, -22.153364439080136, -26.76438979842235, -31.293892800864242, -35.74642741022849, -40.1263243867305, -44.43769458993596, -48.684433746986, -52.87022844076772, -56.99856303254738, -61.07272725334383, -65.09582423697746, -69.07077880867638, -73.00034588029466, -76.88711883493433, -80.73353781009595, -84.54189781003367, -88.31435659548967, -92.05294231312352, -95.75956083832611, -99.43600281421973, -103.08395037689536, -106.70498356267649, -110.30058639769035, -113.87215267351058, -117.42099141529135, -120.94833205080201, -124.45532929021823, -127.94306772753548, -131.41256617513358, -134.86478174340485, -138.3006136775199, -141.7209069634007, -145.12645571482605, -148.51800635335312, -151.89626059242053, -155.26187823662355, -158.61547980674072, -161.95764900065285, -165.28893499984383, -168.60985463071387, -171.92089438947747, -175.2225123389649, -178.51513988520435, -181.79918344122603, -185.07502598511545, -188.3430285189393, -191.60353143478036, -194.85685579375198, -198.10330452350905, -201.34316353943888, -204.57670279439978, -207.80417726157413, -211.0258278547183, -214.24188228982675, -217.45255589197254, -220.65805235084957, -223.85856442831954, -227.05427462105385, -230.2453557811651, -233.43197169753554, -236.61427764037848, -239.7924208714014, -242.9665411217895, -246.13677104008582, -249.30323661190417, -252.46605755329347, -255.62534767944615, -258.7812152503405, -261.933763294797, -265.0830899143377, -268.2292885681436, -271.3724483403233, -274.5126541906258, -277.649987189657, -280.78452473958936, -283.9163407812941, -287.04550598875807, -290.1720879515974, -293.29615134642404, -296.417758097774, -299.53696752925777, -302.6538365055537, -305.7684195658224, -308.8807690490831, -311.9909352120604, -315.09896633997334, -318.20490885071007, -321.30880739280457, -324.4107049376011, -327.5106428659709, -330.6086610499197, -333.7047979294058, -336.7990905846624, -339.8915748043088, -342.9822851495035, -346.07125501439106, -349.15851668306425, -352.2441013832602, -355.3280393369899, -358.4103598082854, -361.49109114824705, -364.5702608375476, -367.6478955265537, -370.7240210732071, -373.79866257879854, -376.8718444217663, -379.94359028963333, -383.0139232091983, -386.0828655750821, -389.150439176731, -392.21666522396424, -395.28156437115666, -398.34515674013426, -401.4074619418591, -404.4684990969754, -407.52828685528317, -410.58684341420184, -413.6441865362834, -416.7003335658282, -419.7553014446587, -422.80910672709626, -425.8617655941895, -428.91329386723413, -431.96370702062785, -435.0130201940963, -438.0612482043249, -441.10840555603187, -444.1545064525129, -447.19956480568715, -450.24359424567103, -453.28660812990927, -456.3286195518838, -459.3696413494256, -462.409686112652, -465.4487661915478, -468.4868937032107, -471.52408053877883, -474.5603383700588, -477.5956786558664, -480.6301126480993, -483.6636513975555, -486.6963057595057, -489.7280863990418, -492.7590037962018, -495.78906825089354, -498.8182898876201, -501.84667866001905, -504.87424435522644, -507.9009965980729, -510.9269448551198, -513.952098438545, -516.9764665098849, -520.0000580836377, -523.0228820307392, -526.0449470819116, -529.0662618308985, -532.0868347375808, -535.1066741309936, -538.1257882122339, -541.1441850572743, -544.1618726196841, -547.1788587332585, -550.1951511145675, -553.2107573654191, -556.2256849752499, -559.2399413234358, -562.2535336815395, -565.2664692154816, -568.2787549876545, -571.2903979589692, -574.3014049908451, -577.3117828471409, -580.3215381960332, -583.3306776118383, -586.3392075767897, -589.3471344827615, -592.3544646329481, -595.361204243499, -598.367359445109, -601.37293628457, -604.3779407262798, -607.3823786537145, -610.3862558708626, -613.389578103625, -616.3923510011809, -619.3945801373163, -622.3962710117277, -625.3974290512886, -628.3980596112888, -631.3981679766467, -634.3977593630896, -637.3968389183102, -640.3954117230965, -643.393482792436, -646.3910570765956, -649.3881394621781, -652.3847347731552, -655.3808477718803, -658.3764831600761, -661.3716455798053, -664.3663396144184, -667.3605697894834, -670.3543405736948, -673.3476563797655, -676.3405215653006, -679.3329404336528, -682.3249172347607, -685.3164561659731, -688.3075613728527, -691.2982369499674, -694.2884869416671, -697.2783153428423, -700.2677260996702, -703.2567231103476, -706.2453102258072, -709.2334912504247, -712.2212699427076, -715.2086500159766, -718.19563513903, -721.1822289367996, -724.1684349909913, -727.1542568407178, -730.1396979831173, -733.1247618739617, -736.1094519282556, -739.0937715208217, -742.0777239868793, -745.0613126226108, -748.044540685718, -751.027411395971, -754.0099279357444, -756.9920934505478, -759.9739110495443, -762.9553838060627, -765.9365147580985, -768.9173069088089, -771.8977632269986, -774.877886647596, -777.8576800721241, -780.8371463691625, -783.8162883748003, -786.7951088930839, -789.7736106964567, -792.751796526191, -795.7296690928132, -798.7072310765227, -801.6844851276027, -804.6614338668272, -807.6380798858567, -810.6144257476334, -813.5904739867651, -816.5662271099063, -819.541687596131, -822.5168578973023, -825.491740438433, -828.4663376180425, -831.4406518085085, -834.4146853564122, -837.3884405828782, -840.3619197839089, -843.3351252307158, -846.3080591700423, -849.280723824485, -852.2531213928066, -855.2252540502485, -858.1971239488332, -861.1687332176668, -864.1400839632342, -867.1111782696914, -870.082018199152, -873.0526057919702, -876.0229430670202, -878.9930320219687, -881.9628746335484, -884.9324728578216, -887.9018286304433, -890.8709438669213, -893.8398204628679, -896.808460294254, -899.7768652176536, -902.7450370704901, -905.7129776712743, -908.6806888198413, -911.6481722975844, -914.6154298676843, -917.5824632753347, -920.5492742479678, -923.5158644954715, -926.4822357104069, -929.4483895682233, -932.4143277274668, -935.3800518299892, -938.3455635011512, -941.3108643500254, -944.2759559695951, -947.2408399369484, -950.2055178134732, -953.1699911450471, -956.1342614622249, -959.0983302804235, -962.0621991001059, -965.0258694069588, -967.9893426720715, -970.9526203521112, -973.9157038894946, -976.8785947125575, -979.8412942357245, -982.8038038596719, -985.7661249714939, -988.72825894486, -991.6902071401762, -994.6519709047404, -997.6135515728978, -1000.5749504661911, -1003.536168893513, -1006.497208151252, -1009.4580695234403, -1012.4187542818967, -1015.3792636863702, -1018.3395989846784, -1021.2997614128466, -1024.2597521952446, -1027.2195725447207, -1030.1792236627355, -1033.1387067394912, -1036.0980229540617, -1039.0571734745226, -1042.0161594580713, -1044.9749820511572, -1047.933642389601, -1050.892141598715, -1053.8504807934246, -1056.808661078386, -1059.766683548099, -1062.7245492870256, -1065.6822593697013, -1068.6398148608457, -1071.597216815476, -1074.5544662790119, -1077.5115642873843, -1080.4685118671432, -1083.4253100355604, -1086.3819598007306, -1089.3384621616783, -1092.2948181084532, -1095.2510286222343, -1098.207094675423, -1101.1630172317427, -1104.1187972463345, -1107.074435665849, -1110.029933428543, -1112.9852914643673, -1115.9405106950605, -1118.8955920342362, -1121.850536387473, -1124.8053446524023, -1127.7600177187912, -1130.7145564686314, -1133.6689617762202, -1136.6232345082453, -1139.577375523867, -1142.5313856747964, -1145.485265805378, -1148.4390167526674, -1151.3926393465085, -1154.3461344096127, -1157.2995027576314, -1160.2527451992344, -1163.2058625361815, -1166.158855563396, -1169.1117250690397, -1172.0644718345793, -1175.0170966348617, -1177.96960023818, -1180.9219834063442, -1183.874246894749, -1186.8263914524396, -1189.7784178221798, -1192.7303267405155, -1195.6821189378413, -1198.6337951384621, -1201.5853560606583, -1204.5368024167467, -1207.4881349131433, -1210.4393542504222, -1213.3904611233784, -1216.3414562210849, -1219.2923402269512, -1222.2431138187842, -1225.193777668842, -1228.1443324438906, -1231.0947788052645, -1234.045117408915, -1236.9953489054697, -1239.9454739402859, -1242.8954931535013, -1245.845407180089, -1248.7952166499083, -1251.744922187758, -1254.6945244134233, -1257.6440239417304, -1260.5934213825938, -1263.542717341066, -1266.4919124173853, -1269.441007207024, -1272.3900023007354, -1275.3388982846025, -1278.287695740081, -1281.2363952440485, -1284.1849973688459, -1287.1335026823265, -1290.081911747894, -1293.0302251245532, -1295.978443366949, -1298.9265670254088, -1301.8745966459853, -1304.8225327705006, -1307.7703759365822, -1310.7181266777088, -1313.6657855232472, -1316.6133529984938, -1319.5608296247128, -1322.5082159191768, -1325.4555123952039, -1328.4027195621936, -1331.34983792567, -1334.2968679873136, -1337.2438102449994, -1340.190665192836, -1343.137433321197, -1346.0841151167604, -1349.030711062542, -1351.9772216379276, -1354.923647318715, -1357.8699885771375, -1360.816245881907, -1363.7624196982395, -1366.7085104878936, -1369.6545187092004, -1372.6004448170959, -1375.5462892631522, -1378.4920524956092, -1381.437734959407, -1384.3833370962152, -1387.3288593444631, -1390.2743021393694, -1393.2196659129752, -1396.1649510941672, -1399.1101581087125, -1402.0552873792872, -1405.000339325498, -1407.9453143639194, -1410.8902129081146, -1413.8350353686665, -1416.7797821532045, -1419.7244536664298, -1422.6690503101438, -1425.613572483274, -1428.5580205818994, -1431.5023949992774, -1434.4466961258672, -1437.3909243493576, -1440.33508005469, -1443.2791636240845, -1446.2231754370632, -1449.1671158704742, -1452.1109852985162, -1455.0547840927654, -1457.9985126221893, -1460.9421712531812, -1463.8857603495755, -1466.8292802726735, -1469.7727313812652, -1472.716114031651, -1475.6594285776648, -1478.6026753706944, -1481.5458547597034, -1484.4889670912528, -1487.4320127095227, -1490.3749919563309, -1493.3179051711563, -1496.2607526911563, -1499.2035348511904, -1502.146251983836, -1505.088904419414, -1508.0314924860008, -1510.9740165094547, -1513.916476813431, -1516.8588737194018, -1519.8012075466747, -1522.7434786124131, -1525.6856872316525, -1528.6278337173183, -1531.5699183802458, -1534.5119415291965, -1537.453903470877, -1540.3958045099553, -1543.3376449490784, -1546.27942508889, -1549.221145228046, -1552.1628056632333, -1555.1044066891857, -1558.0459485986971, -1560.987431682644, -1563.9288562299969, -1566.8702225278362, -1569.811530861371, -1572.7527815139526, -1575.69397476709, -1578.635110900465, -1581.5761901919489, -1584.5172129176167, -1587.4581793517602, -1590.3990897669062, -1593.339944433827, -1596.2807436215583, -1599.2214875974119, -1602.1621766269893, -1605.1028109741974, -1608.0433909012602, -1610.9839166687345, -1613.924388535522, -1616.864806758884, -1619.8051715944525, -1622.7454832962476, -1625.6857421166858, -1628.6259483065946, -1631.5661021152284, -1634.506203790275, -1637.4462535778748, -1640.3862517226273, -1643.3261984676085, -1646.2660940543794, -1649.2059387230001, -1652.1457327120402, -1655.0854762585914, -1658.025169598281, -1660.9648129652812, -1663.9044065923206, -1666.843950710695, -1669.7834455502832, -1672.7228913395531, -1675.6622883055734, -1678.6016366740282, -1681.540936669223, -1684.4801885140998, -1687.4193924302442, -1690.358548637899, -1693.2976573559722, -1696.2367188020482, -1699.1757331923986, -1702.1147007419927, -1705.0536216645057, -1707.9924961723293, -1710.9313244765835, -1713.8701067871245, -1716.808843312554, -1719.747534260231, -1722.686179836279, -1725.6247802455957, -1728.563335691865, -1731.5018463775625, -1734.4403125039669, -1737.3787342711687, -1740.3171118780785, -1743.2554455224383, -1746.1937354008255, -1749.1319817086674, -1752.0701846402455, -1755.008344388706, -1757.9464611460694, -1760.8845351032346, -1763.8225664499923, -1766.7605553750323, -1769.6985020659474, -1772.6364067092468, -1775.5742694903622, -1778.5120905936535, -1781.4498702024212, -1784.3876084989104, -1787.325305664321, -1790.2629618788146, -1793.2005773215194, -1796.1381521705437, -1799.075686602978, -1802.0131807949042, -1804.9506349214053, -1807.8880491565674, -1810.8254236734927, -1813.7627586443023, -1816.7000542401458, -1819.6373106312078, -1822.5745279867149, -1825.5117064749397, -1828.4488462632146, -1831.3859475179304, -1834.3230104045492, -1837.260035087607, -1840.1970217307241, -1843.1339704966078, -1846.0708815470618, -1849.0077550429903, -1851.944591144407, -1854.88139001044, -1857.818151799337, -1860.7548766684727, -1863.6915647743563, -1866.6282162726343, -1869.564831318099, -1872.5014100646965, -1875.4379526655257, -1878.3744592728513, -1881.3109300381066, -1884.2473651118992, -1887.1837646440174, -1890.1201287834347, -1893.0564576783186, -1895.9927514760313, -1898.9290103231401, -1901.86523436542, -1904.8014237478599, -1907.7375786146672, -1910.6736991092764, -1913.6097853743486, -1916.5458375517824, -1919.481855782717, -1922.417840207536, -1925.3537909658753, -1928.2897081966257, -1931.2255920379396, -1934.161442627234, -1937.097260101199, -1940.033044595799, -1942.9687962462792, -1945.9045151871724, -1948.8402015522988, -1951.7758554747763, -1954.7114770870214, -1957.6470665207557, -1960.5826239070093, -1963.5181493761277, -1966.453643057774, -1969.389105080933, -1972.3245355739184, -1975.2599346643767, -1978.195302479288, -1981.1306391449762, -1984.0659447871087, -1987.0012195307013, -1989.9364635001252, -1992.8716768191093, -1995.8068596107435, -1998.7420119974854, -2001.6771341011615, -2004.6122260429754, -2007.5472879435058, -2010.4823199227171, -2013.4173220999592, -2016.352294593972, -2019.2872375228906, -2022.2221510042496, -2025.1570351549842, -2028.0918900914378, -2031.026715929361, -2033.9615127839222, -2036.8962807697028, -2039.8310200007102, -2042.7657305903736, -2045.7004126515515, -2048.6350662965356, -2051.5696916370543, -2054.5042887842724, -2057.438857848802, -2060.373398940697, -2063.307912169465, -2066.2423976440673, -2069.176855472921, -2072.1112857639023, -2075.0456886243533, -2077.9800641610836, -2080.914412480372, -2083.848733687971, -2086.783027889113, -2089.7172951885063, -2092.6515356903465, -2095.5857494983143, -2098.519936715581, -2101.4540974448128, -2104.388231788167, -2107.3223398473083, -2110.2564217233944, -2113.190477517097, -2116.1245073285922, -2119.058511257569, -2121.9924894032297, -2124.9264418642965, -2127.8603687390105, -2130.794270125135, -2133.728146119963, -2136.661996820316, -2139.595822322545, -2142.5296227225385, -2145.4633981157212, -2148.3971485970605, -2151.3308742610657, -2154.264575201793, -2157.1982515128443, -2160.131903287378, -2163.0655306181043, -2165.999133597289, -2168.932712316762, -2171.866266867909, -2174.799797341684, -2177.7333038286115, -2180.66678641878, -2183.600245201854, -2186.5336802670736, -2189.4670917032536, -2192.4004795987917, -2195.3338440416674, -2198.2671851194436, -2201.200502919273, -2204.133797527899, -2207.067069031653, -2210.0003175164647, -2212.9335430678607, -2215.8667457709653, -2218.799925710506, -2221.733082970815, -2224.6662176358277, -2227.5993297890923, -2230.532419513767, -2233.4654868926195, -2236.398532008037, -2239.331554942024, -2242.2645557762035, -2245.1975345918213, -2248.130491469747, -2251.0634264904793, -2253.99633973414, -2256.929231280487, -2259.862101208907, -2262.7949495984253, -2265.7277765277004, -2268.6605820750333, -2271.593366318364, -2274.5261293352764, -2277.458871203, -2280.3915919984106, -2283.3242917980333, -2286.256970678045, -2289.189628714275, -2292.122265982209, -2295.0548825569876, -2297.9874785134125, -2300.920053925946, -2303.85260886871, -2306.7851434154977, -2309.717657639763, -2312.6501516146277, -2315.58262541289, -2318.515079107016, -2321.447512769143, -2324.3799264710897, -2327.3123202843503, -2330.244694280096, -2333.177048529184, -2336.1093831021467, -2339.0416980692103, -2341.97399350028, -2344.9062694649533, -2347.838526032516, -2350.770763271944, -2353.702981251909, -2356.6351800407765, -2359.567359706608, -2362.499520317163, -2365.431661939902, -2368.3637846419865, -2371.295888490281, -2374.227973551355, -2377.160039891482, -2380.0920875766487, -2383.024116672544, -2385.956127244575, -2388.8881193578554, -2391.820093077218, -2394.752048467209, -2397.68398559209, -2400.6159045158447, -2403.5478053021757, -2406.479688014506, -2409.4115527159843, -2412.3433994694824, -2415.275228337597, -2418.2070393826566, -2421.1388326667147, -2424.0706082515567, -2427.0023661987, -2429.934106569397, -2432.8658294246316, -2435.7975348251266, -2438.72922283134, -2441.660893503472, -2444.5925469014596, -2447.5241830849845, -2450.4558021134685, -2453.3874040460814, -2456.318988941736, -2459.2505568590927, -2462.18210785656, -2465.113641992298, -2468.045159324216, -2470.9766599099758, -2473.9081438069925, -2476.839611072439, -2479.7710617632392, -2482.7024959360797, -2485.6339136474016, -2488.5653149534087, -2491.4966999100643, -2494.4280685730955, -2497.359420997992, -2500.290757240008, -2503.2220773541653, -2506.153381395252, -2509.084669417824, -2512.015941476208, -2514.947197624501, -2517.8784379165727, -2520.809662406062, -2523.740871146391, -2526.672064190746, -2529.6032415921004, -2532.5344034031964, -2535.4655496765613, -2538.3966804644992, -2541.327795819097, -2544.258895792224, -2547.1899804355294, -2550.121049800452, -2553.0521039382106, -2555.9831428998154, -2558.9141667360627, -2561.845175497535, -2564.7761692346076, -2567.707147997446, -2570.6381118360055, -2573.5690608000396, -2576.499994939088, -2579.4309143024916, -2582.3618189393846, -2585.292708898699, -2588.2235842291648, -2591.1544449793114, -2594.085291197467, -2597.016122931761, -2599.9469402301265, -2602.8777431402973, -2605.808531709812, -2608.7393059860165, -2611.6700660160586, -2614.600811846894, -2617.53154352529, -2620.462261097817, -2623.3929646108586, -2626.3236541106085, -2629.25432964307, -2632.1849912540624, -2635.115638989215, -2638.0462728939733, -2640.9768930135974, -2643.9074993931645, -2646.8380920775658, -2649.7686711115143, -2652.699236539539, -2655.62978840599, -2658.5603267550373, -2661.4908516306714, -2664.421363076707, -2667.351861136781, -2670.2823458543544, -2673.2128172727125, -2676.143275434966, -2679.073720384053, -2682.00415216274, -2684.9345708136193, -2687.8649763791122, -2690.795368901472, -2693.725748422782, -2696.656114984955, -2699.5864686297377, -2702.516809398711, -2705.4471373332863, -2708.3774524747128, -2711.307754864073, -2714.238044542288, -2717.1683215501125, -2720.0985859281423, -2723.0288377168104, -2725.9590769563874, -2728.8893036869868, -2731.8195179485606, -2734.749719780905, -2737.679909223654, -2740.6100863162906, -2743.540251098137, -2746.470403608362, -2749.400543885979, -2752.330671969847, -2755.2607878986737, -2758.190891711011, -2761.120983445262, -2764.0510631396764, -2766.9811308323547, -2769.911186561248, -2772.841230364156, -2775.771262278735, -2778.701282342488, -2781.6312905927734, -2784.5612870668047, -2787.491271801647, -2790.421244834223, -2793.351206201311, -2796.281155939543, -2799.211094085412, -2802.141020675264, -2805.0709357453075, -2808.0008393316098, -2810.9307314700955, -2813.86061219655, -2816.790481546623, -2819.720339555821, -2822.650186259515, -2825.5800216929406, -2828.5098458911943, -2831.439658889238, -2834.3694607218986, -2837.2992514238663, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
            "cubic - [22.587232318814884, -121.38916898019608, 33.699200125997535, -63.223119718625426, -381.91667045276256, -283.30015477285036, -658.7184534518443, -716.4242030248564, -716.0065320158466, -762.2356359743619, -1010.3794376592716, -1073.0477672388197, -1154.1763866067542, nan, -1361.9387431546988, nan, -1577.897487274185, nan, -1766.9911613010788, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:56: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:56: RuntimeWarning: divide by zero encountered in log\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HV6DRoDxG1Rl"
      },
      "source": [
        "# Grader's area\n",
        "\n",
        "maxScore = maxScore +12\n",
        "#M[1,1] = \n",
        "#M[1,2] = \n",
        "#M[1,3] ="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------------\n",
        "-----------------------\n",
        "-----------------------\n"
      ],
      "metadata": {
        "id": "6grYJ9spn6C1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zW8T7xf0-uUL"
      },
      "source": [
        "## <font color = 'blue'> **Question 2. A theoretical question** </font>\n",
        "\n",
        "This question is about a theoretical explanation for what you observed in question 1(iii). \n",
        "\n",
        "<br>\n",
        "\n",
        "Suppose $f_1$ is a model that optimally fits the data $(X,y)$, and $f_2$ is another model that optimally fits the data $(X_2,y)$ where $X_2$ are the quadratic features of $X$. Then the loss function value obtained by $f_2$ is **always** going to be at least equal to that for $f_1$. Try to come up with a solid mathematical argument that justifies this claim. [Note: as with anything else, feel free to discuss this on Canvas]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2 Answer**\n",
        "\n",
        "For linear with datapoints (x1,x2)\n",
        "\n",
        "y_hat = w1.x1 + w2.x2 + b\n",
        "\n",
        "loss = -y.log y_hat - (1-y)log(1-y_hat)\n",
        "\n",
        "For quadratic with datapoints (x1,x2)\n",
        "\n",
        "y_hat_quad = w1.x1 + w2.x2 + w3.x1_sqr + w4.x2_sqr + w5.x1.x2 + b\n",
        "\n",
        "loss = -y.log y_hat_quad - (1-y)log(1-y_hat_quad)\n",
        "\n",
        "When compared to linear, only number of features are increased in quadratic model.\n",
        "If the weights w3, w4, w5 are minimal (or when <=0), loss function will be almost equal to linear model\n",
        "i.e., \n",
        "y_hat_quad = w1.x1 + w2.x2 + b (where w3,w4,w5 <=0)\n",
        "Computing loss with y_hat_quad will be same as the loss values obtained in linear model\n",
        "\n"
      ],
      "metadata": {
        "id": "nRk7QbZLH_DY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJrBDgJYBQZL"
      },
      "source": [
        "# Grader's area\n",
        "\n",
        "maxScore = maxScore + 4\n",
        "#M[2,1] = \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------------\n",
        "-----------------------\n",
        "-----------------------\n"
      ],
      "metadata": {
        "id": "hTvgj_V9seFm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMEcdAp3-uT-"
      },
      "source": [
        "##  <font color = 'blue'> **Question 3. Logistic Regression Fail?**  </font>\n",
        "\n",
        "Unlike the perceptron, logistic regression does **not** guarantee that the output will actually separate 2-label data points that are linearly separable. Construct a 2-dimensional data set (i.e. the data points and their labels), with the properties that: <br> \n",
        "\n",
        "**a.** the data set is linearly separable  <br>\n",
        "**b.** the optimal logistic regression model does not separate the data.  <br>\n",
        "\n",
        "Demonstrate your answer as follows: <br>\n",
        "\n",
        "<font color = 'blue'> **Q3-1**.  </font>\n",
        " Plot the data points, as we did above for the iris data set. This will show that your data set is linearly separable. <br>\n",
        "<font color = 'blue'> **Q3-2**.  </font> Calculate the optimal logistic neuron weights using the function *LogisticRegressionGD* from question 1. <br>\n",
        "<font color = 'blue'> **Q3-3**. </font>  Find one point in your data set, and show the calculations that prove that it is misclassified by the optimal logistic neuron. \n",
        "\n",
        "\n",
        "**Hint**: Try small datasets.\n",
        "**Note**: It's best to use fresh variables for your dataset, because the previous $X,y$ are re-used in question 4.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# your answers go here\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Generating random datapoints (scale values obtained based on trials)\n",
        "x_arange1 =  np.arange(2,3,0.02)\n",
        "x_arange2 =  np.arange(2,3,0.02)\n",
        "x_rand1 = x_arange1 +  np.random.rand(x_arange1.shape[0])\n",
        "x_rand2 = x_arange2 + 6*np.random.rand(x_arange2.shape[0])+5 \n",
        "x_new = np.vstack((np.column_stack((x_rand2,x_rand1)) ,np.column_stack((x_rand1,x_rand1))))\n",
        "print(x_new.shape)\n",
        "y_new = np.hstack((np.ones(45)*-1.0,np.ones(55)*1.0))"
      ],
      "metadata": {
        "id": "-dh1NMTf2P62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b1e84b2-ce8b-42b0-d759-9d87819ad866"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q3-1. Plot the data points, as we did above for the iris data set. This will show that your data set is linearly separable.\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(\n",
        "    x_new, y_new, test_size=0.3, random_state=1, stratify=y_new)\n",
        "\n",
        "sc = StandardScaler()\n",
        "sc.fit(X_train_new)\n",
        "X_train_std = sc.transform(X_train_new)\n",
        "X_test_std = sc.transform(X_test_new)\n",
        "X_combined_std = sc.transform(x_new)\n",
        "lgr = LogisticRegressionGD(0.001,1000,random_state=1)\n",
        "#y = np.ones(X.shape[0])*-1\n",
        "lgr.fit(x_new, y_new)\n",
        "print(X_combined_std.shape)\n",
        "plot_decision_regions(X=x_new, y=y_new,classifier=lgr)\n",
        "plt.legend(loc=\"upper left\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "O6y03nltgYd_",
        "outputId": "1e0bd71a-0175-4052-825f-8dd7179d52f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f26e68b4f50>"
            ]
          },
          "metadata": {},
          "execution_count": 383
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above plot we could see that logistic regression, could not plot the decision boundary correctly even though the data is seperable"
      ],
      "metadata": {
        "id": "WmrbtEoDRMyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q3-2. Calculate the optimal logistic neuron weights using the function LogisticRegressionGD from question 1.\n",
        "eta_test = 1\n",
        "for i in range(100):\n",
        "  eta_test = eta_test/10\n",
        "  lgr = LogisticRegressionGD(eta_test,1000,random_state=1)\n",
        "  lgr.fit(x_new, y_new)\n",
        "  print(f\"eta - {eta_test} , weights - {lgr.w_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZlOrFbK7xHl",
        "outputId": "377a12f9-9841-47e8-c3a9-7e7b43c2c81d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eta - 0.1 , weights - [   995.11676069 -25573.12567367   3355.79922674]\n",
            "eta - 0.01 , weights - [   99.52629517 -2557.31807319   335.57516911]\n",
            "eta - 0.001 , weights - [   9.95572133 -255.77131916   33.51962956]\n",
            "eta - 0.0001 , weights - [  0.97414508 -25.72857304   3.24108018]\n",
            "eta - 1e-05 , weights - [ 0.0752737  -2.73262563  0.21100639]\n",
            "eta - 1.0000000000000002e-06 , weights - [ 0.00274309 -0.38019539 -0.04168728]\n",
            "eta - 1.0000000000000002e-07 , weights - [ 0.01283402 -0.05972276 -0.01512867]\n",
            "eta - 1.0000000000000002e-08 , weights - [ 0.01585874 -0.01185646 -0.00639835]\n",
            "eta - 1.0000000000000003e-09 , weights - [ 0.01620451 -0.00669554 -0.0053948 ]\n",
            "eta - 1.0000000000000003e-10 , weights - [ 0.01623955 -0.0061754  -0.00529304]\n",
            "eta - 1.0000000000000003e-11 , weights - [ 0.01624306 -0.00612335 -0.00528285]\n",
            "eta - 1.0000000000000002e-12 , weights - [ 0.01624341 -0.00611814 -0.00528183]\n",
            "eta - 1.0000000000000002e-13 , weights - [ 0.01624345 -0.00611762 -0.00528173]\n",
            "eta - 1.0000000000000002e-14 , weights - [ 0.01624345 -0.00611757 -0.00528172]\n",
            "eta - 1e-15 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000001e-16 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1e-17 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1e-18 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000001e-19 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000001e-20 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000001e-21 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1e-22 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000001e-23 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000001e-24 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000002e-25 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000002e-26 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000002e-27 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000002e-28 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000002e-29 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000003e-30 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000003e-31 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000003e-32 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000004e-33 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000004e-34 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000004e-35 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000004e-36 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000005e-37 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000005e-38 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000004e-39 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000003e-40 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000004e-41 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000004e-42 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000003e-43 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000003e-44 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000003e-45 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000002e-46 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000002e-47 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000003e-48 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000003e-49 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000004e-50 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000003e-51 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000004e-52 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000004e-53 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000003e-54 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000004e-55 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000004e-56 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000004e-57 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000004e-58 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000005e-59 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000005e-60 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000006e-61 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000005e-62 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000005e-63 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000005e-64 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000006e-65 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000005e-66 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000004e-67 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000005e-68 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000005e-69 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000005e-70 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000005e-71 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000005e-72 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000005e-73 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000005e-74 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000006e-75 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000005e-76 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000005e-77 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000005e-78 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000004e-79 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000005e-80 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000005e-81 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000005e-82 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000006e-83 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000006e-84 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000005e-85 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000006e-86 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000006e-87 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000006e-88 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000006e-89 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000006e-90 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000007e-91 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000007e-92 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000008e-93 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000008e-94 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000008e-95 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000007e-96 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000007e-97 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000008e-98 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000008e-99 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n",
            "eta - 1.0000000000000008e-100 , weights - [ 0.01624345 -0.00611756 -0.00528172]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above results we could see after certain iteration, weight vectors remains the same, so the optimal weights are [ 0.01624345 -0.00611756 -0.00528172]"
      ],
      "metadata": {
        "id": "e0rBMeWjZv-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q3-3. Find one point in your data set, and show the calculations that prove that it is misclassified by the optimal logistic neuron.\n",
        "lgr = LogisticRegressionGD(0.00001,1000,random_state=1)\n",
        "#y = np.ones(X.shape[0])*-1\n",
        "lgr.fit(x_new[40:100], y_new[40:100])\n",
        "print(\"Predictions from LogisticRegressionGD method..\")\n",
        "print(lgr.predict(x_new[40:100]))\n",
        "\n",
        "print(\"Predictions from manual calc using the weights computed above..\")\n",
        "w = [ 0.01624345, -0.00611756] #weights from above calc\n",
        "y_new_predicted =  np.dot(w,np.transpose(x_new[40:100]))\n",
        "y_new_hat = np.where(y_new_predicted >= 0,1,-1)\n",
        "#print(y_new_predicted)\n",
        "print(y_new_hat)\n",
        "\n",
        "print(\"From the above two predictions, we can see few data points were misclassified..\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wmbyXwCeIIA",
        "outputId": "3de107bd-9c18-4aba-de38-dc48ea2064c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions from LogisticRegressionGD method..\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "Predictions from manual calc using the weights computed above..\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "From the above two predictions, we can see few data points were misclassified..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayVRy0pxlBDP"
      },
      "source": [
        "# Grader's area\n",
        "\n",
        "maxScore = maxScore + 12\n",
        "#M[3,1] = \n",
        "#M[3,2] = \n",
        "#M[3,3] = \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------------\n",
        "-----------------------\n",
        "-----------------------\n"
      ],
      "metadata": {
        "id": "kffsOAaln8C_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  <font color = 'blue'> **Question 4. SVM and margin classification**  </font>\n",
        "\n",
        "The Iris dataset defined in above cells is linearly separable. \n",
        "\n",
        "<font color = 'blue'> **Q4-1.**  </font> Use a [linear SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html) to learn a hyperplane $y=w_1x_1 +w_2x_2 +b$ that maximizes the margin for this Iris dataset. In your answer, specify a setting for the hyperparameter $C$ that reduces the amount of regularization, i.e. it incentivizes small slacks. \n",
        "\n",
        "<font color = 'blue'> **Q4-2.** </font> Calculate the margin for the hyperplane you computed above.  [Hint: Read the documentation in order to access the coefficients, and then use these coefficients to calculate the margin]\n",
        "\n",
        "**Note:** as with anything else, feel free to discuss this on Canvas."
      ],
      "metadata": {
        "id": "uVUZKNB2kod6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "#svm = SGDClassifier(loss='hinge')"
      ],
      "metadata": {
        "id": "AfJ4CzwpUTs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,100):\n",
        "  c = i+1\n",
        "  #Q4-1. Use a linear SVC to learn a hyperplane y=w1x1+w2x2+b that maximizes the margin for this Iris dataset.\n",
        "  print(f\"Linear SVC in Iris dataset with hyper parameter C: {c}\")\n",
        "  X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.15) \n",
        "  clf = make_pipeline(StandardScaler(),\n",
        "                    LinearSVC(random_state=0, tol=1e-5,C=c,loss='hinge'))\n",
        "  clf.fit(X, y)\n",
        "  coef_ = clf.named_steps['linearsvc'].coef_\n",
        "  #print(clf.named_steps['linearsvc'].coef_)\n",
        "  print(f\"Coefficient: {coef_}\")\n",
        "  print(f\"Intercept: {clf.named_steps['linearsvc'].intercept_}\")\n",
        "  # calculate margin\n",
        "  print(f\"Margin: {2.0 /np.sqrt(np.sum(coef_ ** 2)) }\\n\")  #Q4-2. Calculate the margin for the hyperplane\n",
        " \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jksafjUvT-o8",
        "outputId": "4acc7b2b-d23e-4c15-ffb6-185675b190b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear SVC in Iris dataset with hyper parameter C: 2\n",
            "Coefficient: [[1.32982216e-05 2.05900786e+00]]\n",
            "Intercept: [0.37429469]\n",
            "Margin: 0.9713416037519628\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 3\n",
            "Coefficient: [[1.58871169e-05 2.05900895e+00]]\n",
            "Intercept: [0.37430498]\n",
            "Margin: 0.9713410898378632\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 4\n",
            "Coefficient: [[1.69204009e-05 2.57368328e+00]]\n",
            "Intercept: [0.71781818]\n",
            "Margin: 0.7770963962218095\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 5\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 6\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 7\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 8\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 9\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 10\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 11\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 12\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 13\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 14\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 15\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 16\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 17\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 18\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 19\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 20\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 21\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 22\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 23\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 24\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 25\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 26\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 27\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 28\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 29\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 30\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 31\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 32\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 33\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 34\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 35\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 36\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 37\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 38\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 39\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 40\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 41\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 42\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 43\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 44\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 45\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 46\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 47\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 48\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 49\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 50\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 51\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 52\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 53\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 54\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 55\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 56\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 57\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 58\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 59\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 60\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 61\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 62\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 63\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 64\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 65\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 66\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 67\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 68\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 69\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 70\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 71\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 72\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 73\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 74\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 75\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 76\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 77\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 78\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 79\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 80\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 81\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 82\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 83\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 84\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 85\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 86\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 87\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 88\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 89\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 90\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 91\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 92\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 93\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 94\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 95\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 96\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 97\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 98\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 99\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n",
            "Linear SVC in Iris dataset with hyper parameter C: 100\n",
            "Coefficient: [[1.25979248e-05 2.62054924e+00]]\n",
            "Intercept: [0.74909642]\n",
            "Margin: 0.763198786730314\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above computation we could see that as C increases, margin decreases. Margin is maximum when C=1\n",
        "\n",
        "Linear SVC in Iris dataset with hyper parameter C: 1\n",
        "\n",
        "Coefficient: [[-0.04266948  1.85548212]]\n",
        "\n",
        "Intercept: [0.39797493]\n",
        "\n",
        "Margin: 1.0776020681333847"
      ],
      "metadata": {
        "id": "Afk3umfGvhQy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64Pwo6Mxmv6X"
      },
      "source": [
        "# Grader's area\n",
        "\n",
        "maxScore = maxScore + 8\n",
        "#M[4,1] = \n",
        "#M[4,2] = \n",
        "#M[4,3] = \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------------\n",
        "-----------------------\n",
        "-----------------------\n"
      ],
      "metadata": {
        "id": "JTLcwDVan9x9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Grader's area\n",
        "\n",
        "rawScore = np.sum(M)\n",
        "score = rawScore*100/maxScore"
      ],
      "metadata": {
        "id": "IVkneTzYCAxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RHdcyebzleMQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}